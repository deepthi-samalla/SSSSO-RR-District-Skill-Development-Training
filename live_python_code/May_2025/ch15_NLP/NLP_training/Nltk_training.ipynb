{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb56d4b4-1be0-4259-a1a3-491c737dc1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "# pip install nltk\n",
    "import spacy\n",
    "# pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29df6970-77d7-4dbd-9ced-20b805390512",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk # Download necessary NLTK resources - \n",
    "            # NLTK is a leading platform for building Python programs to work with human language data.\n",
    "            # NLTK full form is Natural Language Toolkit.\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize # Tokenization\n",
    "from nltk.corpus import stopwords # Stopwords, common words to ignore\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer # Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cf7d2935",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')  # Download sentence tokenizer models\n",
    "# punkt is a pre-trained model for tokenizing text into sentences and words.\n",
    "# punkt means \"punctuation\" and is used to identify sentence boundaries.\n",
    "nltk.download('punkt_tab') # punkt_tab is a tab-separated version of the punkt tokenizer, useful for specific applications.\n",
    "nltk.download('stopwords') # Download stopwords corpus\n",
    "nltk.download('wordnet') # Download WordNet corpus for lemmatization\n",
    "\n",
    "# only need to run once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ce39ece-82f3-4fb6-8abe-f08e0427fb50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello there!', 'How are you doing today?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to E:\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "save_path = r'E:\\nltk_data'\n",
    "nltk.data.path.append(save_path)\n",
    "\n",
    "nltk.download('punkt', download_dir=save_path)\n",
    "nltk.data.path.append(save_path)\n",
    "\n",
    "# Example 2: Sentence tokenization\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"Hello there! How are you doing today?\"\n",
    "sentences = sent_tokenize(text)\n",
    "print(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2db1b55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n",
      "['Hello', 'there', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "# Difference between nltk and spacy:\n",
    "# # NLTK is a comprehensive library for NLP tasks, while spaCy is designed for production use with a focus on speed and efficiency.\n",
    "# NLTK provides more linguistic data and tools, while spaCy offers pre-trained models and pipelines for quick deployment.\n",
    "\n",
    "# What is common between nltk and spacy:\n",
    "# Both NLTK and spaCy are popular libraries for natural language processing in Python, \n",
    "# providing tools for tokenization, part-of-speech tagging, named entity recognition, and more.\n",
    "# They can be used together in a project to leverage the strengths of both libraries.\n",
    "\n",
    "\n",
    "import spacy\n",
    "spacy.cli.download(\"en_core_web_sm\")\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f9ab5f4f-eace-4de1-8968-e4af00bd1892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'there', '!', 'How', 'are', 'you', 'doing', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Word tokenization\n",
    "words = word_tokenize(text)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fad08949-b81e-4706-a5ae-915fd5587a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'yours', \"didn't\", 'did', 'herself', \"mightn't\", 'most', 'll', \"shouldn't\", 'are', 'ma', 'few', \"won't\", \"he's\", 'whom', 'when', 'against', 'of', \"wouldn't\", 'no', \"he'd\", 'its', \"she'd\", \"hadn't\", \"you'd\", \"wasn't\", 'all', 'weren', 'until', 'themselves', \"should've\", 'was', 'more', 'into', \"shan't\", 'himself', 'be', 'by', 'other', \"it'd\", 'up', 'don', 'couldn', 'out', 'm', \"isn't\", 'very', 'too', 'aren', 'each', 'nor', 'shan', 'down', 'it', 'than', 'is', 'such', 'from', 'these', 'theirs', 'while', 'isn', 's', \"aren't\", \"it's\", \"that'll\", 'at', \"they'll\", \"doesn't\", 'o', 'them', 'as', 'hasn', 'been', 'their', 'own', 'only', 'both', 'hadn', 'my', 'there', 'not', 'were', 'can', 'i', 'with', 'yourselves', 'or', 'who', 'a', \"haven't\", 'am', 'needn', \"we'd\", 'mustn', 'shouldn', 'being', 'where', 'does', 'had', 'about', 'will', 'her', \"you've\", 'and', 'what', 'that', 'which', 'won', 'doesn', 'ain', \"we're\", 'me', 'd', 'so', \"she'll\", 'haven', 'hers', 'didn', 'again', 'here', \"you're\", 'mightn', 'further', 'some', 'having', \"they'd\", \"they've\", \"we've\", 've', 'before', 'how', 'above', 'his', 'do', 'should', 'for', \"you'll\", 'you', 'on', \"she's\", \"he'll\", 're', 'just', \"weren't\", 'our', 'an', 'those', 'but', \"i'll\", 'under', \"couldn't\", 'he', 'through', 'below', 'why', 'off', \"don't\", 'your', 'we', \"it'll\", 'yourself', 'between', \"i've\", 'to', 'wasn', \"i'd\", 'myself', 'ourselves', \"needn't\", \"we'll\", 'she', 'have', 'in', 'wouldn', 'itself', 'over', 'during', 'they', 'doing', 'y', 'then', \"hasn't\", 'same', 'this', 'because', \"i'm\", 'after', \"mustn't\", \"they're\", 't', 'ours', 'him', 'the', 'now', 'has', 'once', 'if', 'any'}\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Stopwords are common words that are often ignored in text processing, such as \"the\", \"is\", \"in\", etc.\n",
    "# They are removed to focus on the more meaningful words in the text.\n",
    "print(stop_words)\n",
    "filtered_words = [w for w in words if w.lower() not in stop_words]\n",
    "\n",
    "\n",
    "# # Example 4: Remove stopwords\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# # Stopwords are common words that are often ignored in text processing, such as \"the\", \"is\", \"in\", etc.\n",
    "# # They are removed to focus on the more meaningful words in the text.\n",
    "# print(stop_words)\n",
    "# # filtered_words = [w for w in stop_words if w.lower() not in stop_words]\n",
    "# # print(filtered_words)\n",
    "# # print(filtered_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a30f8ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', '!', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Example 5: Stemming\n",
    "ps = PorterStemmer()\n",
    "stemmed_words = [ps.stem(w) for w in filtered_words]\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "43f4a076-6ab0-448c-9005-1c57e7f1079c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', '!', 'today', '?']\n"
     ]
    }
   ],
   "source": [
    "# Example 6: Lemmatization\n",
    "# Lemmatization is the process of reducing a word to its base or root form.\n",
    "# It considers the context and converts the word to its meaningful base form.\n",
    "# For example, \"running\" becomes \"run\", \"better\" becomes \"good\".\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_words = [lemmatizer.lemmatize(w) for w in filtered_words]\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d00ab2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 'NNP'), ('there', 'EX'), ('!', '.'), ('How', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('today', 'NN'), ('?', '.')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Example 7: POS tagging\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "pos_tags = nltk.pos_tag(words)\n",
    "print(pos_tags)\n",
    "\n",
    "# [('Hello', 'NNP'), ('there', 'EX'), ('!', '.'), ('How', 'WRB'), ('are', 'VBP'), ('you', 'PRP'), ('doing', 'VBG'), ('today', 'NN'), ('?', '.')]\n",
    "# NNP: Proper noun, singular\n",
    "# EX: Existential there\n",
    "# WRB: Wh-adverb (e.g., where, when)\n",
    "# VBP: Verb, non-3rd person singular present\n",
    "# PRP: Personal pronoun\n",
    "# VBG: Verb, gerund or present participle\n",
    "# NN: Noun, singular or mass\n",
    "# .: Punctuation mark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11733841",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\admin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (GPE Hello/NNP)\n",
      "  there/EX\n",
      "  !/.\n",
      "  How/WRB\n",
      "  are/VBP\n",
      "  you/PRP\n",
      "  doing/VBG\n",
      "  today/NN\n",
      "  ?/.)\n"
     ]
    }
   ],
   "source": [
    "# Example 8: Named Entity Recognition (NER)\n",
    "# NER is the process of identifying and classifying named entities in text, such as people, organizations, locations, etc.\n",
    "# Example: \"Barack Obama was the 44th President of the United States.\" \n",
    "# NER would identify \"Barack Obama\" as a person and \"United States\" as a location.\n",
    "\n",
    "# NLTK provides a named entity chunker that can be used for this purpose.\n",
    "nltk.download('maxent_ne_chunker_tab')\n",
    "nltk.download('words')\n",
    "from nltk import ne_chunk\n",
    "tree = ne_chunk(pos_tags)\n",
    "print(tree)\n",
    "\n",
    "# The output will be a tree structure where named entities are labeled \n",
    "# with their types (e.g., PERSON, ORGANIZATION, GPE for geopolitical entity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfcb7411",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Hello', 1), ('there', 1), ('!', 1), ('How', 1), ('are', 1)]\n"
     ]
    }
   ],
   "source": [
    "# Example 9: Frequency distribution of words\n",
    "freq_dist = nltk.FreqDist(words)\n",
    "print(freq_dist.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8993fa0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying 1 of 1 matches:\n",
      "     Hello there ! How are you doing today ?\n"
     ]
    }
   ],
   "source": [
    "# Example 10: Concordance\n",
    "text_obj = nltk.Text(words)\n",
    "text_obj.concordance(\"today\") # Finds occurrences of the word \"today\" in the text\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
