{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef7e62c3-0159-48c1-8581-6b2e82d17932",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'NLP', \"'s\", 'tokenization', 'is', 'essential', 'for', 'text', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer\n",
    "\n",
    "text = \"Hello world! NLP's tokenization is essential for text analysis.\"\n",
    "\n",
    "# Example 1: Basic word tokenization\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "80049112-700a-4952-b9ca-e886371d82f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Do', \"n't\", 'hesitate', 'to', 'ask', 'questions', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Tokenizing contractions correctly\n",
    "text2 = \"Don't hesitate to ask questions.\"\n",
    "print(word_tokenize(text2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba909cf-c667-4a19-a531-5b1894b8f3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'NLP', 's', 'tokenization', 'is', 'essential', 'for', 'text', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Using RegexpTokenizer to keep only words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "print(tokenizer.tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b0c91b5b-3374-4c52-a268-63640b6e43f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', '!', 'NLP', \"'s\", 'tokenization', 'is', 'essential', 'for', 'text', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Using SpaCy tokenizer\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(text)\n",
    "print([token.text for token in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f45a2341-52d1-48e1-b486-12bb89312ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world', '!', 'nlp', \"'s\", 'tokenization', 'is', 'essential', 'for', 'text', 'analysis', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example 5: Tokenize and convert to lowercase\n",
    "tokens_lower = [t.lower() for t in tokens]\n",
    "print(tokens_lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b748cc-bd10-4982-b5c0-db60c39e7141",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['State-of-the-art', 'NLP', 'techniques', 'are', 'evolving', '.']\n"
     ]
    }
   ],
   "source": [
    "# Example 6: Tokenize words including hyphens\n",
    "text3 = \"State-of-the-art NLP techniques are evolving.\"\n",
    "print(word_tokenize(text3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61bd8cf4-be40-40d0-8e93-5339d5adf795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'world', 'NLP', 'tokenization', 'is', 'essential', 'for', 'text', 'analysis']\n"
     ]
    }
   ],
   "source": [
    "# Example 8: Removing punctuation tokens\n",
    "tokens_no_punc = [t for t in tokens if t.isalpha()]\n",
    "print(tokens_no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f1f76ad-6bb6-4eda-ae49-ca0b06504c1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hallo', 'Welt', '!', 'Wie', 'geht', \"'s\", '?']\n"
     ]
    }
   ],
   "source": [
    "# Example 9: Tokenizing multilingual text\n",
    "text_de = \"Hallo Welt! Wie geht's?\"\n",
    "print(word_tokenize(text_de, language='german'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "50f468e1-4dbb-4a9c-b62a-d7336aa67506",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'love', 'NLP', 'ðŸ˜Š', '#', 'AI']\n"
     ]
    }
   ],
   "source": [
    "# Example 10: Tokenizing emoji and special characters (basic)\n",
    "text5 = \"I love NLP ðŸ˜Š #AI\"\n",
    "print(word_tokenize(text5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfda72ee-8730-48b3-93e9-00bf78e3c4c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
